---
created: 2024-10-09
modified: 2024-10-18
tags:
  - 📑
aliases: 
parents: 
title: 
---
## 第1節　AIの進化に伴う課題と現状の取組
AIは我々の生活に便利さをもたらす一方で、活用にあたっては留意すべきリスクや課題も存在している。以下に生成AIが抱えるリスク・課題を技術的／社会・経済的な観点からざっくり見ていく。
### 1　生成AIが抱える課題
2024年4月に総務省・経済産業省が策定した「AI事業者ガイドライン（第1.0版）」では、生成AIによって顕在化したリスクについて例示している。

| リスク                   | 事例                                                                                                                  |
| --------------------- | ------------------------------------------------------------------------------------------------------------------- |
| バイアスのある結果および差別的な結果の出力 | - IT企業が自社で開発したAI人材システムが女性を差別するという機械学習の欠陥を持ち合わせていた                                                                   |
| フィルターバブルおよびエコーチェンバー   | - SNS等のレコメンドを通じた社会の分断が生じている                                                                                         |
| 多様性の喪失                | - 社会全体が同じモデルを、同じ温度感で使った場合、導かれる意見および回答がLLMによって収束してしまい、多様性が失われる可能性がある                                                 |
| 不適切な個人情報の取り扱い         | - 透明性を欠く個人情報の利用および個人情報の政治利用も問題視されている                                                                                |
| 生命、身体、財産の侵害           | - AIが不適切な判断を下すことで、自動運転車が事故を引き起こし、生命や財産に深刻な損害を与える可能性がある<br>- トリアージにおいては、AIが順位を決定する際に倫理的なバイアスを持つことで、公平性の損失等が生じる可能性がある |
| データ汚染攻撃               | - AIの学習実施時およびサービス運用時には学習データへの不正データ混入、サービス運用時ではアプリケーション自体を狙ったサイバー攻撃等のリスクが存在する                                        |
| ブラックボックス化、判断に関する説明の要求 | - AIの判断のブラックボックス化に起因する問題も生じている<br>- AIの判断に関する透明性を求める動きも上がっている                                                       |
| エネルギー使用量および環境の負荷      | - AIの利用拡大により、計算リソースの需要も拡大しており、結果として、データセンターが増大しエネルギー使用量の増加が懸念されている                                                  |
| 悪用                    | - AIの詐欺目的での利用も問題視されている                                                                                              |
| 機密情報の流出               | - AIの利用においては、個人情報や機密情報がプロンプトとして入力され、そのAIからの出力等を通じて流出してしまうリスクがある                                                     |
| ハルシネーション              | - 生成AIが事実と異なることをもっともらしく回答する「ハルシネーション」に関してはAI開発者・提供者への訴訟も起きている                                                       |
| 偽情報、誤情報を鵜呑みにすること      | - 生成AIが生み出す誤情報を鵜呑みすることがリスクとなりうる<br>- ディープフェイクは、各国で悪用例が相次いでいる                                                        |
| 著作権との関係               | - 知的財産権の取り扱いへの議論が提起されている                                                                                            |
| 資格等の関係                | - 生成AIの活用を通じた業法免許や資格等の侵害リスクも考えうる                                                                                    |
| バイアスの再生成              | - 生成AIは既存の情報に基づいて回答を作るため既存の情報に含まれる偏見を増幅し、不公平や差別的な出力が継続／拡大する可能性がある                                                   |

#### （1）　主要なLLMの概要
![[各モデルのパラメータ数.png]]
生成AIの基盤となる大規模言語モデル（LLM）の開発では、MicrosoftやGoogleなどアメリカのビックテック企業などが先行している状況にある。

日本以外の企業・研究機関がクローズに研究開発を進めたLLMを活用するだけでは、LLM構築の過程がブラックボックス化してしまい、LLMを活用する際の権利侵害や情報漏洩などの懸念を払拭できない。

日本語に強いLLMの利活用のためには、国産のLLM構築が必要となる。すでに日本の企業においても、独自LLM開発に取り組んでおり、ここではその動向を紹介する。

日本では、中規模モデルのLLMが開発されている傾向が見られる。
#### （2）　国産LLMの開発
##### ア NICTによる国産LLMの開発
2023年7月、国立研究開発法人情報通信研究機構（NICT）は、400億パラメータの生成系の大規模言語モデルを開発した旨を発表した。[^nict]NICTの開発したLLMについてはファインチューニングや強化学習は未実施であり、性能面ではChatGPT等と比較できるレベルではない。しかし日本語でのやりとりができる水準に到達しているとしており、今後は学習テキストについて、日本語を中心としてさらに大規模化していくこととしている。

また、GPT-3と同規模の1,790億パラメータのモデルの事前学習に取り組み、適切な学習の設定等を探索していく予定である。

[^nict]: [日本語に特化した大規模言語モデル（生成AI）を試作｜2023年｜NICT-情報通信研究機構](https://www.nict.go.jp/press/2023/07/04-1.html)
##### イ サイバーエージェントが開発した日本語LLM「CyberAgentLM」
2023年5月、サイバーエージェントが最大68億パラメータの日本語LLMを開発したことを発表した。[^cyber]2023年11月には、より高性能な70億パラメータ、32,000トークン対応の日本語LLM「CyberAgentLM2-7B」と、チャット形式でチューニングを行った「CyberAgentLM2-7B-Chat」の種類を公開した。[^cyber2]日本語の文章として約50,000文字相当の大容量テキストを処理可能である。

>[!info]
>2024年7月には「CyberAgentLM3」を一般公開している。[^cyber3]

[^cyber]: [サイバーエージェント、最大68億パラメータの日本語LLM（大規模言語モデル）を一般公開 ―オープンなデータで学習した商用利用可能なモデルを提供― | 株式会社サイバーエージェント](https://www.cyberagent.co.jp/news/detail/id=28817)
[^cyber2]: [独自の日本語LLM（大規模言語モデル）のバージョン2を一般公開 ―32,000トークン対応の商用利用可能なチャットモデルを提供― | 株式会社サイバーエージェント](https://www.cyberagent.co.jp/news/detail/id=29479)
[^cyber3]: [独自の日本語LLM（大規模言語モデル）のバージョン3を一般公開 ―225億パラメータの商用利用可能なモデルを提供― | 株式会社サイバーエージェント](https://www.cyberagent.co.jp/news/detail/id=30463)

##### ウ 日本電信電話(NTT)が開発した日本語LLM「tsuzumi」
2023年11月にNTTが開発した、軽量かつ世界トップレベルの日本語処理能力を持つLLMモデル「[tsuzumi](https://www.rd.ntt/research/LLM_tsuzumi.html)」が発表された。「tsuzumi」のパラメータサイズは6〜70億と軽量であり、クラウド提供型LLMの課題である学習やチューニングに必要なコストを減らせる。「tsuzumi」は英語と日本語に対応しているほか、視覚や聴覚などのモーダルに対応し、特定の業界や企業組織に特化したチューニングができる。

### 2　生成AIが及ぼす課題
生成AIの進展・普及には、それに伴う社会的・経済的な課題も多い。
#### （1）　偽・誤情報の流通・拡散等の課題及び対策
近年、世界各国でディープフェイク[^deepfake]による情報操作や犯罪利用が増えている。その対策には各方面からの取り組みが行われているものの、いたちごっこになっている。

[^deepfake]: 「ディープラーニング（深層学習）」と「フェイク（偽物）」を組み合わせた造語で、AI技術を用いて合成された音声、画像あるいは動画コンテンツを指す。真実と錯覚させるように表示し、人々が実際に発言・行動していない言動を行なっているかのような描写をすることを特徴とする。

##### ア ディープフェイクによる課題
###### (ア) AIにより生成された偽・誤情報の流通・拡散
我が国でも、生成AIを利用して作られた岸田総理大臣の偽動画がSNS上で拡散した事例が発生した。[^kishida]

2024年1月1日に発生した能登半島地震の際にも、東日本大震災の時の津波映像や静岡県熱海市で2021年に起きた大規模土石流の映像などをあたかも能登半島地震と結びつけた投稿がSNSに投稿され、大量に閲覧・拡散された。[^noto]

>[!info]
>>偽情報の投稿者を分析すると、多くはXの有料アカウントであることが分かった。閲覧回数に応じ広告収益を受け取ることができ、虚偽と知りながら注目を集めやすい情報を流している可能性もある。  
>> \- [能登半島地震の偽映像、SNSで拡散　送金募集も - 日本経済新聞](https://www.nikkei.com/article/DGXZQOCA020JZ0S4A100C2000000/)
>
>「お金がもらえるなら社会全体の信用を毀損しても構わない」という考えで行動する人が多い。

2020年には、新型コロナウィルス感染症と5G電波との関係を謳う偽情報が携帯電話基地局の破壊活動を招くなど社会的影響も生じさせている。[^europe]

>[!info]
>「陰謀論の正否について調べれば調べるほど、陰謀論にのめり込むようになる」旨のブログ記事を読んだことがある。これは陰謀論を調べれば調べるほど、その論を補強する情報ばかり出てきてしまうためだという。検索エンジンのアルゴリズムが悪い方向に働いた結果。

>[!info]
>一方で、物事を直感で判断する人ほど陰謀論にハマりやすい、という研究結果もある。（[「頭の良い人」は陰謀論にハマるか、学術誌に論文が掲載…「面白くない」研究結果は心理学者を奮い立たせた : 読売新聞](https://www.yomiuri.co.jp/national/20231101-OYT1T50199/)より）

[^kishida]: [生成ＡＩで岸田首相の偽動画、ＳＮＳで拡散…ロゴを悪用された日テレ「到底許すことはできない」 : 読売新聞](https://www.yomiuri.co.jp/national/20231103-OYT1T50260/)
[^noto]: [能登半島地震の偽映像、SNSで拡散　送金募集も - 日本経済新聞](https://www.nikkei.com/article/DGXZQOCA020JZ0S4A100C2000000/)
[^europe]: [欧州5G基地局破壊、影の犯人は「コロナ拡散」のデマ - 日本経済新聞](https://www.nikkei.com/article/DGXMZO58443970U0A420C2XR1000/)

SNSなどさまざまなデジタルサービスが普及し、あらゆる主体が情報の発信者となった。その結果、インターネット上では膨大な情報やデータが流通するようになった。  
このような情報過多の社会においては、供給される情報量に比して、我々が支払えるアテンションないし消費時間が希少となる。そのため、それらが経済的価値を持って市場で流通するようになる。

このことはアテンション・エコノミーと呼ばれる。プラットフォーム事業者が、受信者のアテンションを得やすい刺激的な情報を優先表示するようになるなど、経済的インセンティブ（広告収入）により偽・誤情報が発信・拡散されたり、インターネット上での炎上を助長させたりする構造となっている。

![[GlobalRisksReport2024.png]]

偽・誤情報の拡散は世界的に問題となっており、2024年1月、世界経済フォーラムは、社会や政治の分断を拡大させるおそれがあるとして、今後2年間で予想される最も深刻なリスクとして「偽情報」を挙げた。[^weforum] 

[^weforum]: [混乱、偽情報、分裂の時代を乗り切るために | 世界経済フォーラム](https://jp.weforum.org/agenda/2024/01/no-wo-ri-rutameni-fo-ramu-sa-dhia-zahidhi/)

特に2024年は50カ国余りで国政選挙が予定されており、生成AIを利用したディープフェイクによる情報操作の事例が確認されている。

###### (イ)その他犯罪利用
OpenAIのチャットボットであるChatGPTに使われているものと同じAIが悪用され、「悪いGPT（BadGPT）」「詐欺GPT（FraudGPT）」と呼ばれる不正チャットボットによってフィッシング詐欺メールが量産されている。

ディープフェイクを利用した犯罪には、AIの画像生成能力を悪用した恐喝行為もある。SNS等で共有された一般的な写真画像をAIで不適切な内容に変換し、被害者を脅迫するというもの。

##### イ ディープフェイクによる情報操作や犯罪利用への対策
###### (ア) 欧州連合(EU)
偽・誤情報に関する法規制で先行するのは欧州連合（EU）である。2022年11月に発効した「デジタルサービス法（DSA、The Digital Services Act）」は、超大規模オンラインプラットフォーム（VLOP）[^vlop]に対して、自身の提供するサービスのリスク評価やリスク軽減措置の実施を義務付けている。違反企業には最大で世界年間売上高の6%の制裁金が科されることとなっている。

実際に、EUの執行機関である欧州委員会(EC)は、X（旧Twitter）がDSAを遵守していない可能性があるとして、違法コンテンツの拡散への対応のほか、プラットフォーム上の情報操作への対抗措置の有効性等の領域について、2023年12月に正式な調査を開始した。[^commision]

2024年3月、欧州議会は、AIに関する世界初の包括的な法的枠組みと位置づける「AI法（AI Act）[^ai-act]」の最終案を可決し、同年5月にEU理事会にて正式承認され、同法が成立した。

[^vlop]: Very large online platform。オンラインプラットフォームサービスのうち、EU域内での利用者が4,500万人（EU域内人口の10%）以上のサービスを指す。
[^commision]: [Commission opens formal proceedings against X under the DSA](https://ec.europa.eu/commission/presscorner/detail/en/ip_23_6709)
[^ai-act]: [AI Act | Shaping Europe’s digital future](https://digital-strategy.ec.europa.eu/en/policies/regulatory-framework-ai)

###### (イ) 英国
イギリスでは、2023年10月に発効された「オンライン安全法（Online Safety Act 2023）[^osa2023]」に、虚偽であると知っている情報を受信者に心理的または身体的危害を与えることを意図してインターネット上で送信した者に、6ヶ月の禁錮刑を刑を科す内容が含まれている。   
特に、相手に苦痛、不安や屈辱等を与える加害意図や、自分が静的満足を得ようとする意図があったと立証されれば、最高刑が懲役2年となる。

[^osa2023]: [Online Safety Act 2023](https://www.legislation.gov.uk/ukpga/2023/50/enacted)

###### (ウ) 米国
アメリカにおいては、2023年7月、バイデン政権が、AI開発を主導する7社[^seven]から、AIの安全性や透明性向上に取り組む自主的なコミットメントを得たと発表した。[^factsheet]  
同年9月には新たに8社[^eight]が合意し、同15社はディープフェイク対策として「電子透かし」等、AIによる生成を識別するための技術開発を推進している。

また、アメリカの一部の州においては、ポルノや選挙活動等の特定の目的化でのディープフェイクに関する規制が見られる。例えば、カリフォルニア、テキサス、イリノイ、ニューヨーク等9州では、相手の同意のないディープフェイクを用いたポルノ画像や動画の配布を刑事犯罪として規定している。そのほか、テキサス州やカリフォルニア州では、公職の候補者に対するディープフェイク等の発信にかかる規制法を設けている。

なお、米国連邦法においては、国防総省や全米科学財団等の連邦機関に対し、ディープフェイクを含む偽情報に関する調査研究の強化等を求める法律が制定されている。  
他方、民間事業者に対しては、1996年成立の「[通信品位法（Communications Decency Act）第230条（通称Section230）](https://ja.wikipedia.org/wiki/%E9%80%9A%E4%BF%A1%E5%93%81%E4%BD%8D%E6%B3%95230%E6%9D%A1)」において、プロバイダは第三者が発信する情報に原則として責任を負わず、有害な内容の削除に責任を問われないと規定されているが、バイデン政権では、偽・誤情報に関してプラットフォーム事業者に一定の責任を求めるよう、法改正しようとする方向で議論が行われている。

[^seven]: Amazon、Anthropic、Google、Inflection、Meta Platforms、Microsoft、OpenAI
[^factsheet]: [FACT SHEET: Biden-Harris Administration Secures Voluntary Commitments from Eight Additional Artificial Intelligence Companies to Manage the Risks Posed by AI | The White House](https://www.whitehouse.gov/briefing-room/statements-releases/2023/09/12/fact-sheet-biden-harris-administration-secures-voluntary-commitments-from-eight-additional-artificial-intelligence-companies-to-manage-the-risks-posed-by-ai/)
[^eight]: Adobe、Cohere、IBM、NVIDIA、Palantir、Salesforce、Scale AI、Stability

###### (エ) 日本
総務省が2023年11月から「[デジタル空間における情報流通の健全性確保の在り方に関する検討会](https://www.soumu.go.jp/main_sosiki/kenkyu/digital_space/index.html)」を開催しており、2024年（令和6年）夏頃までに一定の取りまとめを公表予定である。

技術的な対策としては、インターネット上のニュース記事や広告などの情報コンテンツに、発信者情報を紐づける[オリジネータープロファイル（OP、Originator Profile）](https://originator-profile.org/ja-JP/)技術の研究開発が進んでいる。

![[SynthetiqVision.png]]

また、国立情報学研究所（NII）がフェイク技術対策に関する研究に早朝から取り組んでおり、2021年9月には、AIにより生成されたフェイク顔画像を自動判定するツール「[SYNTHETIQ VISION: Synthetic video detector](https://research.nii.ac.jp/~iechizen/synmediacenter/synthetiq/index.html)」を開発した。

現在NNIでは、さらに進んだディープフェイク対策技術「Cyber Vaccine（サイバーワクチン）」を開発中であり、これが実現すると、真贋判定だけでなく、どこが改竄されたのか等の情報も得ることができるようになると期待されている。

#### （2）　著作権を含む知的財産権等に関する議論
生成AIにおいて、データを収集・複製し、学習用データセットを作成したり、データセットを学習に利用して、AI（学習済みモデル）を開発することがオリジナルデータの制作者等の権利を侵害しないかという開発・学習段階の論点がある。

生成AIを利用して画像を生成・アップロードして公表、生成画像の複製物を販売する際に、既存の画像等の作品と類似したものを使ってしまう等の場合に、既存作品の制作者の権利の侵害等になることがある（生成・利用段階の論点）。

##### ア 生成AIの進展・普及に伴う著作権を含む知的財産権等に関わる問題提起
アメリカでは2022年11月、GitHub Copilotの開発に関連して、学習に使っているオープンソースコードがプログラマーの著作権を侵害している可能性があるとして、Microsoft、GitHub、OpenAIに対する集団訴訟が提訴された。[^github]

[^github]: [GitHub Copilot litigation · Joseph Saveri Law Firm & Matthew Butterick](https://githubcopilotlitigation.com/)

2023年7月には、アメリカの作家3名がOpenAIとMeta Platformsの2社を提訴した訴訟も発生した。同訴訟の結果、OpenAIは学習データから著作物を削除するのではなく、著作権侵害で訴えられた場合の訴訟費用を負担することを表明することとなった。[^openai]

>[!info]
>作家3名の訴訟についてはその後の2024年2月、[「直接的な著作権侵害」「カルフォルニア州不公正競争法(UCL)違反」以外の訴えが棄却された。](https://gigazine.net/news/20240214-openai-win-authors-copyright-lawsuit/)

[^openai]: [LLM litigation · Joseph Saveri Law Firm & Matthew Butterick](https://llmlitigation.com/)

新聞社、通信社等のメディアでのAIの活用は慎重なものとなっている。

アメリカのAssociated Press（AP通信）は2023年7月にOpenAIとの提携を発表し、生成AIをニュース報道に生かす方法等について共同で研究する契約を結んだが、8月にはAIを配信可能なコンテンツ作成のために使用しないとした。  
一方、New York TimesはAIによる記事の無断使用でOpenAIとMicrosoftを訴えた。[^newyork]

>[!info]
>OpenAIはNew York Timesの訴えに対し、「[NYTがプロンプトを意図的に操作して記事を盗用させた](https://gigazine.net/news/20240228-openai-newyork-times-chatgpt-hacking/)」と反論している。これに対しNew York Timesは「[そもそもChatGPTが有料記事をただ読みしている](https://gigazine.net/news/20240313-new-york-times-openai-chatgpt-bypassing-paywalls/)」と反論している。

[^newyork]: [The New York Times、MicrosoftとOpenAIをAIによる著作権侵害で提訴 - ITmedia NEWS](https://www.itmedia.co.jp/news/articles/2312/28/news100.html)

日本では、権利者やAI開発者から著作権などの知的財産権の侵害に関数る懸念の声が上がったことを踏まえ、2024年3月、文化審議会著作権分科会法制度小委員会において、「[AIと著作権に関する考え方について](https://www.bunka.go.jp/seisaku/bunkashingikai/chosakuken/pdf/94037901_01.pdf)」がとりまとめられた。

それとともに知的財産権との関係について、2024年5月、AI時代の知的財産権検討会より、「[AI時代の知的財産権検討会 中間とりまとめ](https://www.kantei.go.jp/jp/singi/titeki2/chitekizaisan2024/0528_ai.pdf)」が公表された。

##### イ 著作権を含む知的財産権等の侵害リスクに対する取組
生成AIの利用に際しての著作権等の権利侵害対策に向けては、データ・コンテンツの権利保持者とAI事業者双方が、互いの契約の中で対応を行うこと等が考えられる。

技術的には電子透かしの実用化や、OpenAIによる知的財産権を侵害する恐れのあるデータ・コンテンツのAI入出力を抑制する仕様の提供等がある。

一方で、New York Times、CNN、Bloomberg、Reuters、日本経済新聞等の国内外のメディア側も、OpenAI等AI事業者のGPTボットのブロックを行う等の対策で自衛している。

技術を活用しながら著作権侵害の法的リスクに対してコミットする取り組みもある。

Microsoftは、大規模言語モデル（LLM）を組み込んだ自社の生産性向上ツール「Microsoft Copilot」に対する法的リスクに対して責任を負う、「[Copilot Copyright Commitment](https://news.microsoft.com/ja-jp/2023/09/12/230912-copilot-copyright-commitment-ai-legal-concerns/)」を2023年9月に発表している。Microsoft Copilotで生成した出力結果を使って、著作権上の意義を申し立てられた場合、Microsoftが責任を取る仕組みとなっている。

Adobeが提供する「[Adobe Firefly](https://www.adobe.com/jp/products/firefly.html)」は、オープンライセンス等、著作権の問題のない画像を学習段階で利用しており、著作権侵害の心配なく生成した画像の商用利用が可能としている。

## 第2節　AIに関する各国の対応
### 1　国際的な議論の動向
#### （1）　広島AIプロセス
AIについての倫理的・社会的課題に対する議論は2015年頃から活発になっている。2016年4月に高松で開催されたG7情報通信大臣会合において、日本はAIの開発原則に関する議論を提案した。その後、OECDで合意されたAI原則が2019年5月に公開されたことを受けて、同年6月のG20首脳会合にて「G20 AI原則」が合意された。

2023年4月、群馬県高崎市でG7群馬高崎デジタル・技術大臣会合が開催され、「責任あるAIとAIガバナンスの推進」などについて議論が交わされた。同会合では、6つのテーマからなる閣僚宣言[^kakuryo]が取りまとめられた。　　
同宣言はその後、5月に広島で開催されたG7広島サミットにおける議論に反映され、広島AIプロセスの創設が支持された。

[^kakuryo]: 6つのテーマは以下のとおり。①越境データ流通と信頼性のある自由なデータ流通(DFFT)の推進、②安全で強靭なデジタルインフラ構築、③自由でオープンなインターネットの維持・推進、④経済社会のイノベーションと新興技術の推進、⑤責任あるAIとAIガバナンスの推進、⑥デジタル市場における競争政策

2023年9月には閣僚級会合が開催され、その後10月30日に「[広島AIプロセスに関するG7首脳声明](https://www.mofa.go.jp/mofaj/files/100573465.pdf)」が発出され、AIシステムの開発者を対象とした国際指針と行動規範が公表された。

#### （2）　OECD／GPAI／UNESCOの動き
##### ア OECD
2019年5月にOECDのAI原則が公開されて以降、各種OECDレポートの発表やプロジェクトの推進等、G7との連携の下、積極的な活動が行われている。

また、OECD、GPAI、UNESCOの3機関は、2023年9月に「生成AI時代の信頼に関するグローバルチャレンジ（[Global Challenge to Build Trust in the Age of Generative AI](https://www.globalchallenge.ai/)）」を発表した。

2024年5月に開催されたOECD閣僚理事会では、生成AIに関するサイドイベント「安全、安心で信頼できるAIに向けて：包括的なグローバルAIガバナンスの促進」において、「[広島AIプロセス フレンズグループ](https://www.soumu.go.jp/hiroshimaaiprocess/supporters.html)」を立ち上げることを発表した。[^side-event]

[^side-event]: [令和6年5月2日 生成ＡＩに関するサイドイベント　岸田総理スピーチ | 総理の演説・記者会見など | 首相官邸ホームページ](https://www.kantei.go.jp/jp/101_kishida/statement/2024/0502speech2.html)

##### イ GPAI (Global Partnership on AI)
「AIに関するグローバルパートナーシップ」（以下GPAI）は、2020年にOECDとG7の共同声明により創設された。同組織は、OECDが事務局を務め、価値観を共有する政府、国際機関、産業界、有識者等からなる官民国際連携組織で、現在29カ国が参加している。[^community]

[^community]: [Community - GPAI](https://gpai.ai/community/)

GPAIには、「責任あるAI」「データ・ガバナンス」「仕事の未来」「イノベーションと商業化」という4つの研究部会が設置されており、専門官いよる議論と実践的な調査が実施されている。

GPAIの年次サミットである「GPAIサミット2023」においては、[GPAI東京専門家支援センター](https://www2.nict.go.jp/gpai-tokyo-esc/)の立ち上げが承認された。[^gpai-center]同センターでは、生成AIに関する調査・分析等のプロジェクトを先行的に実施する予定となっている。

[^gpai-center]: [GPAI東京専門家支援センターの設置について｜2024年｜NICT-情報通信研究機構](https://www1.nict.go.jp/press/2024/07/01-2.html)

##### ウ UNESCO（国連教育科学文化機関）
UNESCOも、2021年にAIの倫理に関する勧告「[UNESCO Recommendation on the Ethics of Artificial Intelligence](https://unesdoc.unesco.org/ark:/48223/pf0000381137)」を採択し、各国における取り組みを支援している。

2023年9月には「[教育・研究分野における生成AIのガイダンス](https://unesdoc.unesco.org/ark:/48223/pf0000386693)」を公表し、生成AIの定義や説明、倫理的および政策的な論点と教育分野への示唆、規制の検討に必要なステップ、カリキュラムデザインや学習等について紹介している。  
ほとんどの生成AIが大人向けに設計されていることから、教育現場での使用は13歳以上に制限すべきと提案し、各国政府には、データのプライバシー保護を含む適切な規制や教員研修等を求めている。

#### （3）　AI安全性サミット
2023年5月、OpenAIは、今後10年以内に人間の専門家のスキルレベルを超えるAIシステムが実現する可能性があると発表した。同社はこれを「フロンティアAI(Frontier AI)」と命名し、事後的対応ではなく国際的な規制を検討すべきとした。

>[!info]
>2024年9月28日、OpenAIのCEOであるサム・アルトマンは「[The Intelligence Age](https://ia.samaltman.com/)」というブログ記事を投稿した。  
>内容は決意表明ともポエムとも取れるものであるが、別の見方をする人もいる。この投稿ができるほど進化したAIをサム・アルトマンはもう手にしたのではないか、というもの。

これを受けてイギリスのスナク首相は、2023年11月1日〜2日に「[AI安全性サミット](https://www.aisafetysummit.gov.uk/)」を開催した。「AI倫理」を超えて「AIの安全性」について議論されたことが特徴的である。

本サミットの成果文書として「ブレッチリー宣言[^bletchley]」が採択された。またイギリスはAIセーフティ・インスティテュートを設置することも決定した。

[^bletchley]: [ブレッチリー宣言で各国が最先端AIの安全かつ責任ある開発に合意 « デイリーウォッチャー｜研究開発戦略センター（CRDS）](https://crds.jst.go.jp/dw/20231207/2023120737112/)

2024年5月21日〜22日には、韓国とイギリスの共催により「AIソウル・サミット」が開催された。首脳級の成果文書として[「安全、革新的で包括的なAIのためのソウル宣言」および付録「AI安全性の化学に関する国際協力に向けたソウル意図表明」](https://www.mofa.go.jp/mofaj/files/100672518.pdf)、閣僚級の成果文書として「[安全、革新的で包括的なAIの発展のためのソウル閣僚声明](https://www.soumu.go.jp/main_content/000948312.pdf)」が採択された。

今後、2025年2月にフランスにて次回会合が開催される予定となっている。

#### （4）　国際連合の動向
2023年7月の国連安全保障理事会においては、イギリス主導でAIに関する議論が行われた。また、2024年3月21日、国連総会において「[持続可能な開発のための安全、安心で信頼できるAIに関する初めての国連決議](https://documents.un.org/doc/undoc/ltd/n24/065/92/pdf/n2406592.pdf)」をコンセンサス[^consensus]で採択し、同決議案は、安全、安心で信頼できるAIに関する初めての国連総会決議となった。

[^consensus]: 意見の一致、合意。ここでは、国連参加国による全会一致の採択を指す。これは「単なる合意はわずかに半数をこえる人々の意見の一致であってもよいが，コンセンサスは社会の構成員の圧倒的多数を含むことを要求される」ため。（[改訂新版　世界大百科事典「コンセンサス](https://kotobank.jp/word/%E3%82%B3%E3%83%B3%E3%82%BB%E3%83%B3%E3%82%B5%E3%82%B9-67361#:~:text=%E3%80%98%20%E5%90%8D%E8%A9%9E%20%E3%80%99%20(%20%5B%E8%8B%B1%E8%AA%9E%5D,%E3%80%81%E5%9B%BD%E6%B0%91%E3%81%AE%E5%90%8C%E6%84%8F%E3%80%81%E8%B3%9B%E5%90%8C%E3%80%82)」より）

同決議案は、広島AIプロセスをはじめ、G7やG20、OECD等で進めてきた議論を反映したものである。国連総会決議には国際法上の拘束力はないものの、コンセンサスで加盟国が採択したということから、国際社会の総意としての政治的な重みを持つものである。

### 2　各国における法規制・ガイドライン等の整備動向
2023年はAI政策にとっては大きな節目となる年となった。
- EUのAI法の欧州議会での採択
- アメリカのAIの安全性に係る大統領令
- 日本のAI関連事業者向けのガイドライン案の公表

#### （1）　欧州連合（EU）
域内発のビッグテック[^big-tech]企業がないヨーロッパは、他の地域に先駆けてもっとも厳しい規制を目指し、2020年からAIの規制に関する議論を続けてきた。

[^big-tech]: 「世界規模で支配的な影響力を持つ巨大IT（情報技術）企業群の通称。一般的には米国のグーグル（Google、現アルファベット傘下）、アップル（Apple）、メタ（Meta、旧Facebook）、アマゾン・ドット・コム（Amazon.com）、マイクロソフト（Microsoft）の5社を指す。」（[野村證券の証券用語解説](https://www.nomura.co.jp/terms/japan/hi/A03358.html)より）

2024年5月21日にはヨーロッパ市場でAIシステムを開発・提供・利用する事業者を対象とする[AI法（AI Act）](https://digital-strategy.ec.europa.eu/en/policies/regulatory-framework-ai)が成立した。2026年頃には本格的に適用される見込みである。

AI法はリスクに応じて規制内容を変える「リスクベースアプローチ」という方針に基づいている。規制対象を4段階のリスクレベルに分類し、それぞれに対して異なる規制を課すこととしている。

![[AI法におけるリスクベースアプローチ.png]]

- 許容できないリスク(Unacceptable Risk)
- 高いリスク(High Risk)
	- 国民の健康や命を危険に晒す可能性があるもの
		- 重要なインフラ（交通など）
	- 人の進路を決めてしまう可能性があるもの
		- 教育または職業訓練（試験やテストの採点）
	- 製品の安全コンポーネント
		- ロボット支援手術におけるAIアプリ
	- 労働者の雇用と管理、自営業へのアクセス
		- 採用手続き用の履歴書仕分けソフトウェア
	- 不可欠な民間・公共サービス
		- 国民の融資機会を奪うような信用スコア
	- 人々の基本的権利を侵害する可能性がある法執行
		- 証拠の信頼性評価
	- 移民、亡命、国境警備
		- ビザ申請の自動審査
	- 司法行政と民主的プロセス
		- 裁判所の判決を検索するAIソリューション
- 限定的なリスク(Limited Risk)
	- AI使用時の透明性の欠如に関連するリスク
	- AIが生成したコンテンツであることを明示
		- チャットボットなどのAIシステム
		- 公共の利益に関する事項を公表するAI生成テキスト
		- ディープフェイク
- 最小限のリスク(Minimal Risk)
	- AI対応のビデオゲーム
	- スパムフィルター

#### （2）　米国
ビッグテック企業を多く保有するアメリカは、政府による規制よりも民間での自主的な対応を優先し、企業の取り組みに任せつつ必要の場合に政府が規制をかけるという立場をとってきた。

民間側の取り組みとして2023年7月、AI開発で先行する7社[^seven]がAIの安全な開発のための自主的な取り組みを約束した。さらに9月には新たな8社[^eight]がそれに合意したことをアメリカ政府が発表した。[^factsheet]  
各社は、自主的なコミットメントとして、安全性[^safety]、セキュリティ[^security]、信頼性[^trust]の3つの観点から原則を掲げている。

[^safety]: システム公開前の安全性確保：各社は、AIシステムをリリースする前に安全性のテストを行う。またAIのリスク管理に関する情報を、産業界、政府、市民社会、学術界と広く共有する。
[^security]: セキュリティを確保したシステムの構築：各社は、独自のモデルや公開前のモデルの重要性を保護するために、セキュリティの確保では、サーバーセキュリティ対策やAI開発に係る知的財産の保護等を行う。また、第三者によるAIシステムの脆弱性の発見と報告を促進する。
[^trust]: 国民の情報の獲得：各社は、AIが生成したコンテンツであることを利用者が知ることができるよう、電子透かしの技術等、堅牢な技術を開発する。また、AIシステムの能力、限界、適切／不適切な使用領域を公表する。

ホワイトハウスはその3ヶ月後となる2023年10月30日、バイデン大統領は「安全・安心・信頼できるAIの開発の利用に関する大統領令」を発表した。[^whitehouse]対象とするAIの問題については、安全保障問題に範囲を拡充しており、対象となる事業者はビッグテック企業に限らず、国家の安全保障や経済に影響を及ぼす可能性のあるサービスや製品を取り扱う企業も含まれる。

[^whitehouse]: [Executive Order on the Safe, Secure, and Trustworthy Development and Use of Artificial Intelligence | The White House](https://www.whitehouse.gov/briefing-room/presidential-actions/2023/10/30/executive-order-on-the-safe-secure-and-trustworthy-development-and-use-of-artificial-intelligence/)

大統領令の発表に引き続き、同年11月には、ハリス副大統領が「安全で責任あるAI利用の新イニシアチブ（New U.S. Initiatives to Advance the Safe and Responsible Use of Artificial Intelligence）」[^us-Initiatives]を発表した。その中で「[米国AI安全研究所（AI Safety Institute）](https://www.nist.gov/aisi)」を設置するとした。

[^us-iInitiatives]: [FACT SHEET: Vice President Harris Announces New U.S. Initiatives to Advance the Safe and Responsible Use of Artificial Intelligence | The White House](https://www.whitehouse.gov/briefing-room/statements-releases/2023/11/01/fact-sheet-vice-president-harris-announces-new-u-s-initiatives-to-advance-the-safe-and-responsible-use-of-artificial-intelligence/)

US AISIは、国立標準技術研究所(NIST)内に設置され、ガイドライン、ツール、ベンチマーク、ベスト・プラクティスを作成し、AIリスクを特定・軽減するレッドチームを含む評価を実施する。

一方、連邦議会でも連邦レベルでのAI規制に関する法案が議論されている。2023年6月には、上院が「安全なイノベーション枠組み（SAFE Innovation Framework）」を提唱し、同年12月までにテーマ別のフォーラムを9回にわたって開催した。[^join]  
他方の下院は、2024年2月、AIに関する超党派のタスクフォースを設立すると発表し、AI政策の指針となる原則や政策提言を含む包括的な報告書を作成する予定となっている。

[^join]: [米上院トップのシューマー議員、AI法案策定に向けた行動枠組み発表(米国) | ビジネス短信 ―ジェトロの海外ニュース - ジェトロ](https://www.jetro.go.jp/biznews/2023/06/5c66b04d70316937.html)
[^kain]: [米下院、AIに関する超党派タスクフォース設立(米国) | ビジネス短信 ―ジェトロの海外ニュース - ジェトロ](https://www.jetro.go.jp/biznews/2024/02/ae216c9caf74dad2.html)

2024年秋に大統領選挙を控えるアメリカでは、生成AIの普及に伴うディープフェイクによる情報操作等の課題に直面し、AI規制の議論が活発化すると予想される。

#### （3）　英国
イギリスはアメリカ・中国に次いでAI研究が盛んな国とされている。AI分野への民間投資額においても、2023年に初めて4位に転落したものの、2019年以来、世界3位を保ってきた。[^global-ai-index]

[^global-ai-index]: [The Global AI Index - Tortoise](https://www.tortoisemedia.com/intelligence/global-ai/#rankings)

現スナク政権は、当面はEUのAI法のような厳格な規制を新たに整備せず、既存の枠組みで柔軟に対処する方針を表明してきた。同方針を踏まえ、イギリス政府が2023年3月に公表した政策文書「[プロイノベーティブな規制手法（A pro-innovation approach to AI regulation）](https://www.gov.uk/government/publications/ai-regulation-a-pro-innovation-approach/white-paper)」が、イギリスのAI規制の基本的な枠組みに位置付けられている。

同文書では、セキュリティ、透明性、公平性、説明責任、争議可能性の観点から5つの原則が掲げられている。AIガバナンスに取り組むに当たっては、「イノベーション促進型の、柔軟で法規制に縛られない、比例的で信頼できる、順応性があり、明確でかつ協力的な」アプローチをとるとしている。

また、2023年11月27日、英国国家サイバーセキュリティセンター（National Cyber Security Centre: NCSC）と米国サイバーセキュリティ・インフラストラクチャー安全保障庁（Cybersecurity and Infrastructure Security Agency: CISA）が中心となり、日本を含む18か国が共同で、AIシステムのセキュリティガイドラインである「[セキュアAIシステム開発ガイドライン(Guidelines for secure AI system development)](https://www.ncsc.gov.uk/collection/guidelines-secure-ai-system-development)」を公表した。

#### （4）　日本
日本は、民主主義や基本的人権等の観点からは欧米と同様の立場である一方、文化や社会規範の差異により、AIに対する社会認識という点では、欧米とは異なる文化圏にいる。

ヨーロッパは法的拘束力の強いハードローを志向している。それに対し、日本は現時点では、民間事業者の自主的な取り組みを重んじるソフトローアプローチを目指している。

総務省のAIネットワーク社会推進会議による「[AI開発ガイドライン](https://www.soumu.go.jp/main_content/000499625.pdf)」が2017年に、「[AI利活用ガイドライン](https://www.soumu.go.jp/main_content/000637097.pdf)」が2019年に公表された。  
また、同年3月に内閣府の統合イノベーション戦略推進会議が決定した、「[人間中心のAI社会原則](https://www8.cao.go.jp/cstp/aigensoku.pdf)」を基にしたガイドラインが策定された。  
続いて2021年7月に経済産業省が公表した「[AI原則実践のためのガバナンス・ガイドライン](https://www.meti.go.jp/shingikai/mono_info_service/ai_shakai_jisso/pdf/20220128_1.pdf)」（2022年1月に改訂）では、AI事業者が実施すべき行動目標が実践例とともに示されている。

2023年5月、政府は「AI戦略会議」を設置し、「[AIに関する暫定的な論点整理](https://www8.cao.go.jp/cstp/ai/ronten_honbun.pdf)」を公表するとともに、各省庁のガイドライン御統合に向けた作業を進めることとされた。

同年9月には、「新AI事業者ガイドライン スケルトン（案）」が示され、政府は「[AI事業者ガイドライン案](https://www.meti.go.jp/shingikai/mono_info_service/ai_shakai_jisso/pdf/20240119_1.pdf)」を公表した。同案では、安全性やプライバシー保護等の10原則を掲げ、人間の意思決定や認知・感情を不当に操作するものは開発させないとしているが、欧米のような一定の法的拘束力を持つものではない。同案はその後、一般からの一件の公募を経て、2024年4月19日に「[AI事業者ガイドライン(第1.0版)](https://www.meti.go.jp/shingikai/mono_info_service/ai_shakai_jisso/pdf/20240419_1.pdf)」として公表された。

また、2023年12月のAI戦略会議において、岸田総理大臣は、アメリカやイギリスと同様に日本にも「[AIセーフティ・インスティテュート(AI Safety Institute)](https://aisi.go.jp/)」を設立すると発表し、2024年2月14日、経済産業省所轄の[情報処理推進機構（Information-technology Promotion Agency: IPA）](https://www.ipa.go.jp/)に設置された。

## 第3節　その他デジタルテクノロジーに関する議論の動向
### 1　メタバース、ロボティクス、自動運転に関する議論の動向
#### （1）　メタバース
総務省が2023年7月に取りまとめた報告書においては、メタバースに関する課題は、「メタバース空間内に係る課題」と「メタバース空間外と関連する課題」の2つに大別されている。[^metaverse]

[^metaverse]: [総務省｜報道資料｜「Web3時代に向けたメタバース等の利活用に関する研究会」 報告書及び意見募集の結果の公表](https://www.soumu.go.jp/menu_news/s-news/01iicp01_02000120.html)

##### メタバース空間内に係る課題
1. アバターに係る課題
2. プラットフォーム間の相互運用性
3. メタバース構築時・利活用時に係る課題
4. データの取得・利用に係る課題

- 上記の課題に対する取り組みの方向性
	- メタバースの理念に関する国際的な共通認識の形成
	- 相互運用性確保に向けた取り組み（標準化等）
	- メタバース関連サービス提供者向けガイドラインの策定

##### メタバース空間外と関連する課題
1. ユーザーインターフェース(UI)／ユーザー体験(UX)に係る課題
2. メタバースの動向／社会的な影響

- 上記の課題に対する取り組みの方向性
	- 市場、技術、ユーザー動向の継続的フォローアップ
	- メタバースとUI/UXの関係等についての調査研究

また、2023年10月から「[安心・安全なメタバースの実現に関する研究会](https://www.soumu.go.jp/main_sosiki/kenkyu/metaverse2/index.html)」を開催している。ユーザーにとってより安心・安全なメタバースを実現することを目的として、「メタバースの原則（1次案）」の検討等が行われてきたところである。

>[!info]
>同研究会は[2024年9月18日に報告書案を作成し、案に対する意見を募集している。](https://www.soumu.go.jp/menu_news/s-news/01iicp01_02000123.html)

国際機関においても、メタバース等の没入型技術について検討されている。たとえばOECDでは、2022年12月に[Global Forum on Technology(GFTech)](https://www.oecd.org/en/networks/global-forum-on-technology.html)の設置を公表している。没入型技術に関するフォーカスグループでの議論は2023年12月から始めており、2024年秋頃に報告書を取りまとめる予定となっている。  
総務省では、2023年10月に国連が主催した「インターネット・ガバナンス・フォーラム(IGF)京都」において、「民主的価値に基づくメタバースの実現」をテーマとしたセッションをOECDと共同開催するなど、国際的な議論に貢献する取り組みを進めている。

#### （2）　ロボティクス
ロボティクスは今まで日本が強みを有する技術である。特に産業用ロボットにおいては世界市場シェアの46%を占めている。日本では労働人口減少が続いている。そのため、ロボティクス活用による生産性の向上、不足する労働力への対応、新たな産業創出等の期待も大きい。

日本は2015年度に「[ロボット新戦略](https://www.kantei.go.jp/jp/singi/keizaisaisei/pdf/robot_honbun_150210.pdf)」を策定し、官民連携による技術開発プロジェクトを実施してきている。ロボット自体やそれを支える個々の技術は進化してきている一方、ロボット導入現場のニーズとの間のギャップにより社会実装が進んでいないという実態もある。

こうした状況を受けて、国立研究開発法人新エネルギー・産業技術総合開発機構(NEDO)は2023年4月、「ロボット分野における研究開発と社会実装の対極的なアクションプラン」を公表した。[^nedo]  
アクションプランにおいては、ロボット活用が期待される8分野[^eight-area]を取り上げ、2030年を目安に短期で求められる施策を「社会実装加速に向けたアクションプラン」、2035年に向けて中長期のインパクト創出を見据えた施策を「次世代技術基盤構築に向けたアクションプラン」として取りまとめている。

[^eight-area]: ものづくり、食品製造、施設管理、小売・飲食、物流倉庫、農業、インフラ維持管理、建築の8分野。
[^nedo]: [「ロボット分野における研究開発と社会実装の大局的なアクションプラン」を公表 | ニュース | NEDO](https://www.nedo.go.jp/news/press/AA5_101639.html)

#### （3）　自動運転技術
政府は、「[デジタル田園都市国家構想総合戦略（2023改訂版）](https://www.cas.go.jp/jp/seisaku/digital_denen/pdf/20231226honbun.pdf)」において、自動運転による地域交通を推進する観点から、関係府省庁が連携し、地域限定型の無人自動運転移動サービスを2025年目処に50ヶ所程度、2027年度までに100ヶ所以上で実現する目標を掲げている。

また、経済産業省の「[デジタルライフライン全国総合整備計画](https://www.meti.go.jp/policy/mono_info_service/digital_architecture/keikaku.pdf)」においては、アーリーハーベストプロジェクト[^early-harvest]のひとつに自動運転サービス支援道の設定が挙げられている。  
2024年度に新東名高速道路の一部区間等において100km以上の自動運転車優先レーンを設定し、自動運転トラックの運航の実現を目指すほか、2025年度までに全国50ヶ所、2027年度までに全国100ヶ所で自動運転車による移動サービス提供を実施できるよう目指すとされている。

[^early-harvest]: Early Harvest。早摘みとも。ここでは、2024年度からの実装に向けた支援策の先行取り組みを意味する。特に「ドローン航路」「自動運転サービス支援道」「インフラ管理のDX」の3つのプロジェクトを指す。

### 2　サイバーセキュリティの確保に関する議論の動向
近年、国際情勢の複雑化により、日本を含む各国において政府機関等を狙ったサイバー攻撃を多く発生している状況にある。それに加え、生成AI等のテクノロジーの悪用によるリスクの拡大も指摘されている。

従来、サイバーセキュリティは主にシステムの可用性や機密性を確保することに焦点が当てられ、ビジネスの連続性や利便性を確保してきた。これとともに、近年では情報の改竄、偽・誤情報の拡散など、情報の中身の完全性、信頼性に関わるリスクも現れている。

国家安全保障戦略において、「民間の重要インフラ等への国境を超えたサイバー攻撃、偽情報の拡散等を通じた情報戦等が恒常的に生起し、有事と平時の境目はますます曖昧になってきている」と指摘するように、[^nss-j]サイバー空間をめぐる脅威はますます深刻化しており、いわば「常時有事」の状況となっているとも言える。

[^nss-j]: [国家安全保障戦略について | 内閣官房ホームページ](https://www.cas.go.jp/jp/siryou/221216anzenhoshou.html)
