---
created: 2024-06-29
modified: 2024-07-30
tags:
  - 💭
aliases: 
parents: "[[💭自分が英語を勉強する理由]]"
---
LLMのような生成AIは、ユーザが使う言語や、AI製造者が集めたデータセットによる文化的なバイアスがかかることがある。

たとえば、「北方領土はどこの国の領土ですか？」という質問を、それぞれ日本語とロシア語で尋ねる。  
日本語で尋ねた場合、LLMは日本語で書かれたデータ（≒日本政府の見解を反映したデータ）を参考にして、「北方領土は日本の領土です」と回答する可能性が高い。  
しかし、ロシア語で尋ねた場合は、LLMはロシア語で書かれたデータ（≒ロシア政府の見解を反映したデータ）を参考にして、「北方領土はロシアの支配下にあります」と回答する可能性が高い。[^可能性が高い]

上記のような言語によるバイアスを、母語のみの話者が感じ取ることは難しい。言語はあくまでも道具でしかないが、生涯で一番長く使い続けている道具だからだ。道具に慣れてしまうと、その道具の利点・欠点に意識が向かなくなる。

また、[[🌐ChatGPT]]や[[Perplexility]]など主要なLLMは英語圏の企業が開発している。そのため、LLMに含まれるデータセットはどうしても日本語より英語が多くなる。[^英語は日本語の10倍]

LLMは通常、データセットが多ければ多いほど性能が上がる。そのため、生成AIには日本語を使うよりも英語を使った方がよりよい回答が得られる。

（sakana.AIのように、生成AI企業が日本を拠点にする事例も出ている。そうなれば日本語のデータセットが他言語よりも増えるかもしれないが、それでも英語を超えることはないだろう）

[^可能性が高い]: これは私個人の憶測である。LLMの仕組み上、同じ質問に対して一文違わず同じ回答が返ってくることはあり得ない。LLMはあくまでも、これまでの質問と回答から「生起確率の高い」文字列を拾い集めているだけだからだ。また、私自身はロシア語が読めないので、ロシア語の回答をGoogleで日本語で機械翻訳した結果が本当に正しいかどうか判断できない。
[^英語は日本語の10倍]: 東京大学松尾研究室の資料によれば、[NTTデータ先端技術株式会社](https://www.intellilink.co.jp/column/ai/2022/072800.aspx)で挙げられている前学習用日本語データを合計しても、トークン数は約0.3Tトークンしかないらしい。これは、Llama2のトークン数（2T）やGPT-4のトークン数（13T）と比べても、わずかな量しかない。