---
created: 2024-01-24
modified: 2024-08-01
tags: []
aliases: []
parents: 
---
LLM（大規模言語モデル）の主流なモデル構造であるTransformerと、その事前学習の仕組みを理解する。

## 目標
- 言語モデルにおけるTransformerの位置付けについて説明できる
- LLMで主流になっているTransformerのモデル構造について説明できる
- LLMの事前学習のパイプラインについて説明できる
- LLMの事前学習のコードを実装できる

## 言語モデルとTransformerの関係
![[Transformerの立ち位置.png]]

## DL以前の代表的な言語モデル
### 条件付き確率を統計的に求める方法
大規模コーパス内の単語列の出現頻度から求める。  
出現回数: #(・)とすると、以下のとおり。
$$p(東京|日本,の,首都,は)=\frac{\#(日本,の,首都,は,東京)}{\#(日本,の,首都,は)}$$
この手法での課題は以下の2つ。
1. データスパースネス問題
	  単語が長くなると、その出現回数#(・)が急速に減少し、条件付き確率の推定が困難になる。
2. 類義語問題
     類義語が個別の事象として扱われてしまう。言い方を微妙に変えただけでも異なる出現頻度として扱われる。（日本の首都は、日本国の首都は、Japanの首都は）

### n-gram言語モデル
直近のn-1個の単語を使って次の単語を予測する。条件付き確率は出現頻度で推定する。
例えばn=3の場合、3-gram言語モデルで単語を予測する。これによりデータスパースネス問題をある程度回避できる。
$$p(東京|日本,の,首都,は)\approx p(東京|首都,は)$$
この手法の課題は「長距離の単語間の関係性を把握しづらい」こと。  
しかしこれはTransformerで解決できるようになった。

### ニューラル言語モデル
![[エンコーダとデコーダ.png]]
- エンコーダ
	- 文（翻訳元言語）の入力機構を持つニューラルネット
- デコーダ
	- 文（翻訳先言語）の出力機構および再帰的入力機構を持つニューラルネット

#### RNN(Recurrent Neural Network)型言語モデル
冒頭の単語から一単語ずつニューラルネットワークに入力して、ニューロンを逐一更新する。パラメータは使い回す。  
代表的モデル: Seq2Seq  
原理的には単語をいくらでも入力・出力できる。

##### 課題1. 単語間の長距離依存性の把握が困難
ニューロンが固定長なので、長文になるとすべての情報を覚えきれない。

##### 課題2. ネットワークが単語方向に深くなるため、学習が不安定（勾配消失、勾配爆発）＆学習が遅い

## Transformer
Googleを中心にした研究チームが2017年に発表。"Attention Is All You Need"という論文で初出。2017年の発表以後、モデルの改良およびスケールによって数多くのNPLベンチマークでState-of-the-Art（現時点の最先端レベル）を達成している。

エンコーダー・デコーダー・アーキテクチャーで最もよく使われるリカレント層を多頭の自己注意（Self-Attention）に置き換えたもの。
GPT-1〜4でもTransformerモデルが採用されている。

アテンション機能を最大限活用することで、RNN型の問題を解決。
- 単語（トークン）間の長距離依存性が把握できるようになった。
- 誤差逆伝播（BackProp）のステップ数が単語数に依存しなくなり（短くなり）、学習の安定化&高速化
- 学習時の並列計算も効率化できたことで大規模化（分散学習）しやすくなった

### Embedding：単語のベクトル変換
テキストをどうやってTransformerに取り込むか？

#### Word Embedding
単語（スパースな情報）を密な（Dense）表現に変換する。Transformerなどのモデルの入力値として扱える。
一般に、学習完了後のWord Embeddingを（次元圧縮して）可視化すると、単語間の関係性を見ることができる。

#### Positional Encoding
Transformerブロックに取り込む前に、Word Embeddingに位置情報を追加。Transformerブロック自身は単語の位置情報を把握できないので事前に埋め込む。Word EmbeddingにPositional Encodingを加法する。（ともに同次元のベクトル）トークンの位置によって異なる値のベクトルをセットするイメージ。
- WE("春") + PE("これは1番目のトークンです")
- WE("は") + PE("これは2番目のトークンです")
- WE("曙") + PE("これは3番目のトークンです")
PE: 以下の式に従って、ベクトル次元ごとに異なる周波のsin波、cos波をセット。
$$p_i = \begin{pmatrix} \sin(i/10000^{2*1/d})\\ \cos(i/10000^{2*1/d})\\ \cdots \\ \sin(i/10000^{2*1/d})\\ \cos(i/10000^{2*1/d})\\  \end{pmatrix} $$

### Multi-Head Attention
アテンション機構：全トークンの類似度を測ることによって、長距離のトークン間の依存関係を把握することを可能にした機構。
ベクトルの次元数が増えるとQとKの内積の値（分散）が増大する傾向になるため、それを抑える役割としてベクトルの次元数（の平方根）で割る。
$Q$: Queryベクトル、$K$:Keyベクトル、$V$:Valueベクトル、$d_k$: ベクトルの次元数
$$Attention(Q, K, V) = softmax(\frac{QK^T}{\sqrt{d_k}})V$$
1. アテンション機構の入力値: トークンのベクトル表現
2. 線形変換（MLP）にてKey、Valueベクトルを作成
3. 第1トークンについて、線形変換（MLP）にて Queryベクトルを作成
4. QueryベクトルとKeyベクトルの内積により、トークン間の類似度=Scoreを測る
5. 類似度をsoftmaxにより正規化（合計値が1になる）。値が大きい方が類似度(=単語間の依存)が強い
6. 類似度とValueベクトルを掛け算する
7. 全ベクトルの総和を取る。Attentionの類似度に従って、Valueベクトルの加重平均を算出。これを第1トークンにおけるAttention機構の出力値とする
8. 1〜7を全トークンで行う

アテンション処理を複数個並列で行う。その後、出力をひとつのベクトルに統合する。1つのトークンがさまざまなトークンに、異なる形式のアテンションをあてることが可能となる。
$$MultiHead(Q, K, V) = Concat(head_1, \cdots, head_h)W^O $$
where $$head_i = Attention(QW_i^Q, KW_i^K, VW_i^V)$$
### Feed Forward
巨大な２階建てのMLP。Key-Valueで蓄積した知識を抽出する機構として考えられる。
$$FFN(x) = \max(0, xW_1 + b_1)W_2 + b_2$$
例えばGPT-3の場合、Feed Forwardの全体に占めるパラメータの割合：66%
- 入力層/出力層の次元数: 12,228
- 中間層の次元数: 12,228 x 4 = 49,152
- 総ブロック数: 96
- つまりFeed Forwardのパラメータ数: 12,228 x 49,152 x 96 $\fallingdotseq$ 116B
	- GPT-3の総パラメータ数: 175B

### Others
#### Add & Norm
- Add: 残差接続(residual connection)
	- 深い層の学習をする時のテクニック
	- Feed Forwardの前後で残差接続
	- Attentionの前後で残差接続
- Norm: レイヤ正規化(Layer Normalization)
	- 隠れ層の次元軸で平均と分散をとり正規化
	- 学習を効率化するテクニック
#### 出力層
- Linear -> Softmax
- 次の単語の生起確率を出力する。
